---
layout: mypost
title: Can You Really Backdoor Federated Learning?
categories: [论文阅读]
---

<table border="1">
    <tr>
        <th>论文英文名字</th>
        <th>Can You Really Backdoor Federated Learning?</th>
    </tr>
    <tr>
        <th>论文中文名字</th>
        <th>你真的可以后门联邦学习吗？</th>
    </tr>
    <tr>
        <td>作者</td>
        <td>Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, H. Brendan McMahan</td>
    </tr>
    <tr>
        <td>来源</td>
        <td>CoRR, November 2019</td>
    </tr>
    <tr>
        <td>年份</td>
        <td>2019 年 11 月</td>
    </tr>
    <tr>
        <td>作者动机</td>
        <td></td>
    </tr>
    <tr>
        <td>阅读动机</td>
        <td></td>
    </tr>
    <tr>
        <td>创新点</td>
        <td></td>
    </tr>
</table>

# 内容总结  

# 后门攻击场景

+ **对手的抽样**
  + 随机采样攻击
  + 固定频率攻击
+ **后门任务**
  + 在后门攻击中，攻击者的目的是让模型在某些子任务上失败。本文允许非恶意客户端在目标任务中正确标注样本。
  + 本文通过将多个选定的“目标客户机”的示例分组来形成后门任务。将目标客户端的数量称为“后门任务数量”，并探讨其对攻击成功率的影响。

# 模型更新中毒攻击

我们基于 (3; 4) 提出的模型替换范式，重点研究模型更新中毒攻击。当在第 t 轮中仅选择一个攻击者（WLOG假设它是客户端 1）时，攻击者试图通过发送回溯模型 `$\omega^*$` 来替换整个模型
`$$\Delta \omega_t^1 = \beta(\omega^*-\omega_t)$$`
其中 `$\beta$` 是提升系数。

![公式2](公式2.png)

如果我们假设模型已经足够收敛，那么它将在 `$\omega^*$` 的一个小邻域内，因此 k > 1 的其他更新 `$\Delta \omega_t^k$` 很小。如果多个攻击者出现在同一回合中，则假定他们可以相互协调并平均划分此更新。

+ **获取后门模型**
  + 一个描述后门任务的集合 `$D_{mal}$`
  + 一个从真实分布生成的训练样本集合 `$D_{trn}$`
+ **获取后门模型**
  + 不受限制的增强后门攻击：用 `$\omega^t$` 初始化，用 `$D_{mal} \cup D_{trn}$` 训练模型。（更新范数大，作为基线）
  + 范数限制后门攻击：通过使用多轮投影梯度下降训练模型，在每一轮中，我们使用无约束训练策略训练模型，并将其投影回 `$\omega^t$` 周围大小为 `$M/\beta$` 的 `$\ell_2$` 球。

# 防御

+ **更新的规范阈值**

假设对手知道阈值 M，因此总是可以返回这个量级的恶意更新。将这种强大优势赋予对手，使得范数边界防御等同于以下范数裁剪方法：

![公式3](公式3.png)

+ **（弱）差分隐私**

首先裁剪更新（如上所述），然后添加高斯噪声。

# 实验

+ 数据集：EMNIST 数据集
+ 模型：具有两层卷积层，一层最大池层和两层密集层的五层卷积神经网络
+ 评估标准：任务准确率

![无约束攻击实验结果](无约束攻击实验结果.png)

![约束攻击实验结果](约束攻击实验结果.png)

图中绿色的线代表后门任务的平均准确率，准确率越高代表后门攻击越有效。

