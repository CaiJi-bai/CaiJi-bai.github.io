---
layout: mypost
title: How to Backdoor Federated Learning
categories: [论文阅读]
---

<table border="1">
    <tr>
        <th>论文英文名字</th>
        <th>How to Backdoor Federated Learning</th>
    </tr>
    <tr>
        <th>论文中文名字</th>
        <th>如何进行联邦学习后门攻击</th>
    </tr>
    <tr>
        <td>作者</td>
        <td>Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, Vitaly Shmatikov</td>
    </tr>
    <tr>
        <td>来源</td>
        <td>23rd AISTATS 2020: Online [Palermo, Sicily, Italy]</td>
    </tr>
    <tr>
        <td>年份</td>
        <td>2020年8月</td>
    </tr>
    <tr>
        <td>作者动机</td>
        <td>联邦学习本质上是易受 model poisoning 攻击的，这是一种本文提出的新的攻击方法。联邦学习给了所有的参与者对最终模型的直接影响能力，因此这个攻击比只针对数据集的投毒攻击更厉害。</td>
    </tr>
    <tr>
        <td>阅读动机</td>
        <td></td>
    </tr>
    <tr>
        <td>创新点</td>
        <td></td>
    </tr>
</table>

# 内容总结  

# 为什么会受到模型中毒攻击

联邦学习通常容易受到后门和其他模型中毒攻击影响的主要原因：
1. 当有数百万参与者进行训练时，不可能确保他们都不是恶意的。
2. 联邦学习的一个假设就是参与者的数据分布是 Non-IID 的，并且为了聚合的安全性，会引入安全聚合，这样就很难进行异常检测了。
3. 现代深度学习模型的巨大容量。 模型质量的传统指标衡量是模型对主要任务的学习程度，而不是它学到的其他内容。 这种额外的容量可用于引入隐蔽后门，而不会对模型的准确性产生重大影响。

# 联邦学习

联邦学习可以从数以万计设备上学习一个聚合的模型，用户数量 n 是非常大的。在第 t 轮，服务端选取 m 个用户组成一个集合 `$S_m$`，然后把当前的联合模型 `$G^t$` 发给他们。在这个过程中 m 的选择需要权衡效率和训练速度。然后被选中的用户基于 `$G^t$` 模型使用算法 1 对本地数据进行训练得到本地模型 `$L^{t+1}$`，并且把差值 `$L^{t+1}-G^t$` 返回给服务器。然后更新完新的模型就是：

![联邦学习全局模型更新](联邦学习全局模型更新.png)

![算法1](算法1.png)

全局学习率 `$\eta$` 控制着模型更新的比例，当 `$\eta=n/m$` 时候，新的模型就是这 m 个用户的平均值了。

# 怎么进行后门攻击实验

+ **攻击目标**

我们的攻击者希望联邦学习产生一个联合模型，该模型在其主要任务和攻击者选择的后门子任务上均达到高精度，并在攻击后的多轮后门子任务上保持高精度。

+ **构建攻击模型**

**朴素方法：**攻击者可以基于后门的数据训练本地模型，根据论文[Gu 2017]，每个 batch 应当同时包含正常数据和后门的数据，这样子模型可以知道这二者的区别。同时，攻击者可以修改本地的学习率和 epochs，这样使得模型可以很好地过拟合。

**模型替换：**在这种方法下，攻击者试图直接把全局更新的模型 `$G^{t+1}$` 替代为想要的模型 X，如下面的公式所示：

![公式2](公式2.png)

由于训练数据是 Non-IID 的，每个本地模型实际上都离 `$G^t$` 很遥远，不妨假设这个攻击者就是第 m 个用户，那么就是有：

![公式3](公式3.png)

当全局模型开始收敛时，差不多会有：`$\sum_{i=1}^{m-1}(L_i^{t+1}-G^t) \approx 0$`，也就是说 `$L_m^{t+1}$` 和最终模型之间大概就是上图的最后结果。

攻击者需要按比例 `$\gamma=\frac{n}{\eta}$` 放大自己的权重使得在 model averaging 阶段，后门可以保留，并且模型被 X 所替代，这在 FL 的每一轮中都有效，尤其在模型快要收敛的时候更加有效。

+ **提高持久性和规避异常检测**

**约束和缩放：**该方法使攻击者能够生成在主要任务和后门任务上都具有高精度的模型，但不会被聚合器的异常检测器拒绝。 直观地说，我们通过使用一个目标函数将异常检测的规避纳入训练中，该目标函数 (1) 奖励模型的准确性，(2) 惩罚偏离聚合器认为“正常”的模型。 根据 Kerckhoffs 原理，我们假设攻击者知道异常检测算法。

![算法2](算法2.png)

算法 2 显示了 constrain-and-scale 方法，这个 loss 添加了一个额外项 `$\mathcal L_{ano}$`：
`$$\mathcal L_{model} = \alpha \mathcal L_{class} + (1-\alpha) \mathcal L_{ano}$$`

因为攻击者的数据包含正常数据和后门数据，`$\mathcal{L}{\operatorname{class}}$` 标识正常数据和后门数据上的准确性，然后 `$\mathcal{L}{\operatorname{ano}}$` 标识任意类型异常检测，比如权重上的 p-norm。超参数 `$\alpha$` 控制比例。

# 实验

+ **图像分类**

Image classification 采用了CIFAR-10 数据集，用了100个参与者，每轮迭代随机选10个用户，用了轻量级的 ResNet18 CNN 模型，有27万个参数，然后介绍了一下怎么模拟 non-iid 情况。每个参与者本地用了两个epochs，并且学习率是0.1。

Backdoors. 假定攻击者想要联合模型对汽车进行分类，同时对含有某一类特征的汽车的需要注入后门使得模型分类错误，对于其他汽车分类正确。相对于pixel-pattern的后门攻击，这个方法不需要攻击者在推测期间对数据进行修正。

作者选了三个特征用于攻击：绿色的车（30个图），有赛车条纹的车（21个图）和背景有竖直条纹的车（12个图），如下图所示。然后坐着对数据进行分割使得仅仅攻击者含有后门特征的数据，当然这不是本质需求。如果后门特征和正常用户的其他特征相似，攻击者依然可以成功攻击，不过联合模型会很快地在迭代过程中忘记后门。然后还介绍了一些其他的细节，需要详细了解的时候可以再回头看原文。

![CIFAR后门](CIFAR后门.png)

+ **单词预测**

词预测（word prediction）也是FL下比较著名的任务，因为训练数据为文本，比较敏感，同时也杜绝中心化的收集（比如打字记录）。然后再NLP中，词预测也是一个典型任务。

作者使用了别人的案例代码（参考论文描述），然后用了2层LSTM模型，有 10^7 个参数，用了 Reddit 数据集，假定每个用户是独立的参与者的情况下，为了保证每个用户足够的数据量，坐着过绿了少于150和多于500个发表内容的用户，这样子就剩下 83,293 个用户，平均每个用户有247次发表内容。在训练数据集中，每一次发表内容被认为是一句话。然后根据别人的实验，本实验中，每一轮迭代随机选了100个用户，每个用户在本地用了2epoch，学习率是20。然后测试数据集为前一个月的5034个发表记录。

Backdoors. 攻击者想要当某个句子以特定单词开头时，预测给定的单词。如下图所示：

![单词预测后门](单词预测后门.png)

这种语义攻击不需要在推理阶段对输入进行修正。即使一个推荐的单词就很可能改变某些用户对某些事情的看法。为了训练模型，训练数据被串成 T_{seq}=64 的长度，每个batch包含20个这样的句子。loss的计算过程如图3(a)所示。

![单词预测后门的模型损失](单词预测后门的模型损失.png)

攻击者的目标是当输入是触发句的时候输出给定单词，如图 3(b) 所示。为了为后门提供不同的上下文，从而提供模型的健壮性，作者保持批处理中的每个序列不变，但将其后缀替换为以所选单词结尾的触发语句。实际上，攻击者教导当前的全局模型 G^t 预测触发语句中的这个单词，而不做任何其他更改。结果模型类似于 G^t，它有助于在主要任务上保持良好的准确性，从而跳过异常检测。

+ **实验结果**

所有的实验用了100轮的FL，如果某一轮选择了多个攻击者，那么他们会把这些数据添加到一个后门模型中。然后 baseline 也在前面讲过了。

![实验结果1](实验结果1.png)

Single-shot attack. 图4(a) 和图 4(c) 表示了single-shot攻击的实验结果，这个攻击大概意思就是说单一轮次只有一个单一的攻击者，并且旨在前5轮迭代中有攻击行为。当攻击者把梯度发过去之后，后门任务的准确度将会是 100%，然后随着迭代的进行，后门任务的准确率会下降，主任务的准确率将不会受到影响。与之对比，传统方法下的攻击方法无法注入后门。

有些后门就会比其他后门更容易注入，比如“有条纹的墙”这个后门就比“绿色的车”效果更好。作者猜测这是因为绿色的车和正常的良好数据的分布会更接近，因此很容易在迭代的过程中被覆盖。同样对于单词预测也有类似的情况。

Repeated attack. 如果攻击者控制了多个用户那么就有更多的攻击方法了，图4(b) 和图4(d) 展示了随着受控制用户比例对注入的影响。给定比例下，本文所提出的方法会比baseline方法具备更高的后门注入准确率。

# 总结

总的来说就是本文提出了一种新的联邦学习攻击模型，并且可以注入语义信息从而不容易被发现模型被注入了后门。但是呢我觉得有这么个问题，根据注入的流程，攻击者（恶意参与者）得知道其他正常参与者的梯度才能构造出当前轮的梯度。那么server是不是每次丢掉最后一个用户的梯度就可以避免这种攻击了。