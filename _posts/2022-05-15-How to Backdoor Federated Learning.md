---
layout: mypost
title: How to Backdoor Federated Learning
categories: [论文阅读]
---

<table border="1">
    <tr>
        <th>论文英文名字</th>
        <th>How to Backdoor Federated Learning</th>
    </tr>
    <tr>
        <th>论文中文名字</th>
        <th>如何后门联邦学习</th>
    </tr>
    <tr>
        <td>作者</td>
        <td>Jack</td>
    </tr>
    <tr>
        <td>来源</td>
        <td>the 12th ACM Workshop</td>
    </tr>
    <tr>
        <td>年份</td>
        <td>2020年10月</td>
    </tr>
    <tr>
        <td>作者动机</td>
        <td>联邦学习本质上是易受 model poisoning 攻击的，这是一种本文提出的新的攻击方法。联邦学习给了所有的参与者对最终模型的直接影响能力，因此这个攻击比只针对数据集的投毒攻击更厉害。</td>
    </tr>
    <tr>
        <td>阅读动机</td>
        <td></td>
    </tr>
    <tr>
        <td>创新点</td>
        <td></td>
    </tr>
</table>

# 内容总结  

# 为什么会受到模型中毒攻击

联邦学习通常容易受到后门和其他模型中毒攻击影响的主要原因：

1. 当有数百万参与者进行训练时，不可能确保他们都不是恶意的。
2. 联邦学习的一个假设就是 device 上的数据分布是 Non-IID 的，并且为了聚合的安全性，会引入安全聚合，这样就很难进行异常检测了。
3. 深度学习的能力很强，即便是在主任务上不引人任何 backdoor，device 还是容易从 subtask 上引入 backdoor。

# 联邦学习

联邦学习可以从数以万计设备上学习一个聚合的模型，这个用户数量 n 是非常非常大的。在第 t 轮，服务端选取 m 个用户组成一个集合 `$S_m$`，然后把当前的联合模型 `$G^t$` 发给他们。在这个过程中 m 的选择需要权衡效率和训练速度。然后被选中的用户基于 `$G^t$` 模型和本地数据训练得到本地模型 `$L^{t+1}$`，并且把差值 `$L{t+1}-Gt$` 返回给服务器。然后更新完新的模型就是：

![联邦学习全局模型更新](联邦学习全局模型更新.png)

全局学习率 `$\eta$` 控制着模型更新的比例，当 `$\eta=n/m$` 时候，新的模型就是这 m 个用户的平均值了。

# 怎么进行后门攻击实验

+ **攻击目标**

我们的攻击者希望联邦学习产生一个联合模型，该模型在其主要任务和攻击者选择的后门子任务上均达到高精度，并在攻击后的多轮后门子任务上保持高精度。

+ **构建攻击模型**

**朴素方法：**攻击者可以基于后门的数据训练本地模型，根据论文[Gu 2017]，每个 batch 应当同时包含正常数据和后门的数据，这样子模型可以知道这二者的区别。同时，攻击者可以修改本地的学习率和 epochs，这样使得模型可以很好地过拟合。

**模型替换：**在这种方法下，攻击者试图直接把全局更新的模型 `$G^{t+1}$` 替代为想要的模型 X，如下面的公式所示：

![公式2](公式2.png)

由于训练数据是 Non-IID 的，每个本地模型实际上都离 `$G^t$` 很遥远，不妨假设这个攻击者就是第 m 个用户，那么就是有：

![公式3](公式3.png)

当全局模型开始收敛时，差不多会有：`$\sum_{i=1}^{m-1}(L_i^{t+1}-G^t) \approx 0$`，也就是说 `$L_m^{t+1}$` 和最终模型之间大概就是上图的最后结果。

攻击者需要按比例 `$\gamma=\frac{n}{\eta}$` 放大自己的权重使得在 model averaging 阶段，后门可以保留，并且模型被 X 所替代，这在 FL 的每一轮中都有效，尤其在模型快要收敛的时候更加有效。

+ **提高持久性和规避异常检测**

**约束和缩放：**该方法使攻击者能够生成在主要任务和后门任务上都具有高精度的模型，但不会被聚合器的异常检测器拒绝。 直观地说，我们通过使用一个目标函数将异常检测的规避纳入训练中，该目标函数 (1) 奖励模型的准确性，(2) 惩罚偏离聚合器认为“正常”的模型。 根据 Kerckhoffs 原理，我们假设攻击者知道异常检测算法。

![算法2](算法2.png)

算法 2 显示了 constrain-and-scale 方法，这个 loss 添加了一个额外项 `$\mathcal L_{ano}$`：
$$\mathcal L_{model} = \alpha \mathcal L_{class} + (1-\alpha) \mathcal L_{ano}$$

因为攻击者的数据包含正常数据和后门数据，`$\mathcal{L}{\operatorname{class}}$` 标识正常数据和后门数据上的准确性，然后 `$\mathcal{L}{\operatorname{ano}}$` 标识任意类型异常检测，比如权重上的 p-norm。超参数 `$\alpha$` 控制比例。