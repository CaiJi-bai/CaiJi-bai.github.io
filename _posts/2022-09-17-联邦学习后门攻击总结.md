---
layout: mypost
title: 联邦学习后门攻击总结
categories: [论文阅读]
---

# 后门攻击的定义

在联邦学习中，后门攻击是意图让模型对具有某种特定特征的数据做出错误的判断，但模型不会对主任务产生影响。

在联邦学习场景下进行后门攻击会比较困难，一个原因就是在服务端进行聚合运算时，平均化之后会很大程度消除恶意客户端模型的影响，另一个原因是由于服务端的选择机制，因为并不能保证被攻击者挟持的客户端在每一轮都能被选取，从而降低了被后门攻击的风险。

# 后门攻击步骤

1. 中毒样本制作（边缘案例后门攻击、分布式后门攻击、模型依赖触发器）
2. 后门模型训练（自适应攻击、神经毒素）
3. 后门模型上传（模型替换）

# 中毒数据生成

原始图像 `$x$`，触发器 `$\delta$`，掩码 `$m$`，中毒图像：

`$\hat{x} = x \odot (1 - m) + \delta \odot m$`

# 后门攻击训练策略

带有后门攻击行为的联邦学习，其客户端可以分为恶意客户端和正常客户端。不同类型的客户端，其本地训练策略各不相同。

### 正常客户端训练

正常客户端的训练算法如下，其执行过程就是常规的梯度下降过程。

第 t 轮正常客户端 i 的训练算法：

---

input:&ensp;全局模型 `$G^t$`  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;学习率: `$\eta$`;  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;本地迭代次数: `$E$`;  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;每一轮训练的样本大小: `$B$`;  
output: 返回模型更新：`$\Delta_i^t$`

利用服务端下发的全局模型参数 `$G^t$`，更新本地模型 `$L_i^t$`：`$L_i^t \leftarrow G^t$`  
for 对每一轮的迭代 i = 1, 2, 3, ..., E，执行下面的操作 do  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;将本地数据切分为大小 B 的 `$\mathcal{B}$` 份  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;for 对每一个 batch `$b \in \mathcal{B}$`  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;执行梯度下降：`$L_i^t \leftarrow L_i^t-\eta\Delta\ell(L_i^t;b)$`  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;计算更新：`$\Delta_i^t=L_i^t-G^t$`  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;end  
end  

---

### 恶意客户端训练

对于恶意客户端的本地训练，主要体现在两个方面：损失函数的设计和上传服务端的模型权重。

+  对于损失函数的设计，恶意客户端训练的目标，一方面是保证在正常数据集和被篡改毒化的数据集中都取得较好的性能；另一方面是保证本地训练的模型与全局模型之间的距离尽量小（距离越小，被服务端判断为异常模型的概率就越小）。 
+  对于上传服务端的模型权重，根据以下公式：`$L_m^{t+1} \approx \frac{n}{\eta}(X-G^t)+G^t$` 可以看出，通过增大异常客户端 `$m$` 的模型权重，使其在后面的聚合过程中，对全局模型的影响和贡献尽量持久。

第 t 轮恶意客户端 j 的训练算法:

---

input:&ensp;全局模型 `$G^t$`  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;学习率: `$\eta$`;  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;本地迭代次数: `$E$`;  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;每一轮训练的样本大小: `$B$`;  
output: 返回模型更新：`$\Delta_j^t$`

利用服务端下发的全局模型参数 `$G^t$`，更新本地模型 `$L_j^t$`：`$L_j^t \leftarrow G^t$`  
损失函数：`$\ell = \ell_{class\_loss} + \ell_{distance\_loss} / \ell_{ano\_loss}$`  
for 对每一轮的迭代 i = 1, 2, 3, ..., E，执行下面的操作 do  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;将本地数据切分为大小 B 的 `$\mathcal{B}$` 份  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;for 对每一个 batch `$b\in \mathcal{B}$`  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;数据集 `$b=\left\{ D_{cln}^m, D_{adv}^m \right\}$` 中包含正常的数据集 `$D_{cln}^m$` 和被篡改毒化的数据集 `$D_{adv}^m $`  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;执行梯度下降：`$L_j^t \leftarrow L_j^t-\eta\Delta\ell(L_j^t;b)$`  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;计算更新：`$\Delta_i^t=L_j^t-G^t$`  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;end  
end  

---

# 方向

联邦学习中后门攻击的防御

# 问题

### 防御

1. 与安全聚合兼容？（元联邦学习）
2. 被自适应攻击绕过？
3. 降低主任务的性能？

### 攻击

1. 如何设计一个高效的触发器？

# 影响因素

1. 攻击者数量
2. 数据分布
3. 辅助信息
4. 客户端是不是静态的（客户端的良性和恶性变化的）

# 想法

1. 如何区分后门更新和良性更新？（找到一个特征，然后构建一个防御框架）
2. 防御最新的后门攻击（分布式后门攻击）？
3. 设计比现有防御更好的方法？
4. 是否和机器学习在数据缺失情况下的后门防御相似？
5. 改进机器学习中后门防御的方法，使其适用于联邦学习中后门攻击的防御？
6. 攻击时间（早期轮次攻击和收敛轮次攻击）
7. 攻击更新的坐标

# 防御方法

### 鲁棒聚合算法

1. Krum
2. Bulyan
3. TrimmedMean
4. Auror
5. RFA
6. FoolsGold

### 

# 论文

### 后门攻击

1. [How to Backdoor Federated Learning](https://caiji-bai.github.io/posts/2022/05/15/How-to-Backdoor-Federated-Learning.html){:target="_blank"}
2. [Attack of the Tails Yes, You Really Can Backdoor Federated Learning](https://caiji-bai.github.io/posts/2022/05/24/Attack-of-the-Tails-Yes,-You-Really-Can-Backdoor-Federated-Learning.html){:target="_blank"}
3. [DBA: Distributed Backdoor Attacks against Federated Learning](https://caiji-bai.github.io/posts/2022/07/16/DBA-Distributed-Backdoor-Attacks-against-Federated-Learning.html){:target="_blank"}
4. [Neurotoxin_Durable Backdoors in Federated Learning](https://caiji-bai.github.io/posts/2022/07/30/Neurotoxin_Durable-Backdoors-in-Federated-Learning.html){:target="_blank"}
5. [Coordinated Backdoor Attacks against Federated Learning with Model-Dependent Triggers](https://caiji-bai.github.io/posts/2022/08/30/Coordinated-Backdoor-Attacks-against-Federated-Learning-with-Model-Dependent-Triggers.html){:target="_blank"}
6. [A highly efficient, confidential, and continuous federated learning backdoor attack strategy](https://caiji-bai.github.io/posts/2022/08/27/A-highly-efficient,-confidential,-and-continuous-federated-learning-backdoor-attack-strategy.html)

### 防御

1. [Can You Really Backdoor Federated Learning?](https://caiji-bai.github.io/posts/2022/05/21/Can-You-Really-Backdoor-Federated-Learning.html)
2. [Defending against Backdoors in Federated Learning with Robust Learning Rate](https://caiji-bai.github.io/posts/2022/05/31/Defending-against-Backdoors-in-Federated-Learning-with-Robust-Learning-Rate.html)
3. [BaFFLe: Backdoor Detection via Feedback-based Federated Learning](https://caiji-bai.github.io/posts/2022/07/19/BaFFLe-Backdoor-Detection-via-Feedback-based-Federated-Learning.html)
4. [FLAME: Taming Backdoors in Federated Learning](https://caiji-bai.github.io/posts/2022/07/26/FLAME_Taming-Backdoors-in-Federated-Learning.html)
5. [Meta Federated Learning](https://caiji-bai.github.io/posts/2022/07/12/Meta-Federated-Learning.html)
6. [Against Backdoor Attacks In Federated Learning With Differential Privacy](https://caiji-bai.github.io/posts/2022/08/02/Against-Backdoor-Attacks-In-Federated-Learning-With-Differential-Privacy.html)
7. [Resisting Distributed Backdoor Attacks in Federated Learning_A Dynamic Norm Clipping Approach](https://caiji-bai.github.io/posts/2022/09/03/Resisting-Distributed-Backdoor-Attacks-in-Federated-Learning_A-Dynamic-Norm-Clipping-Approach.html)
8. [FederatedReverse: A Detection and Defense Method Against Backdoor Attacks in Federated Learning](https://caiji-bai.github.io/posts/2022/09/06/FederatedReverse_A-Detection-and-Defense-Method-Against-Backdoor-Attacks-in-Federated-Learning.html)
9. [BatFL_Backdoor Detection on Federated Learning in e-Health](https://caiji-bai.github.io/posts/2022/09/10/BatFL_Backdoor-Detection-on-Federated-Learning-in-e-Health.html)
10. [Backdoor attacks-resilient aggregation based on Robust Filtering of Outliers in federated learning for image classification](https://caiji-bai.github.io/posts/2022/09/13/Backdoor-attacks-resilient-aggregation-based-on-Robust-Filtering-of-Outliers-in-federated-learning-for-image-classification.html)
11. [Defense against backdoor attack in federated learning](https://caiji-bai.github.io/posts/2022/09/20/Defense-against-backdoor-attack-in-fe-derate-d-learning.html)
12. [Mitigating the Backdoor Attack by Federated Filters for Industrial IoT Applications](https://caiji-bai.github.io/posts/2022/10/01/Mitigating-the-Backdoor-Attack-by-Federated-Filters-for-Industrial-IoT-Applications.html)