---
layout: mypost
title: 联邦学习后门攻击总结
categories: [论文阅读]
---

# 后门攻击的定义

在联邦学习中，后门攻击是意图让模型对具有某种特定特征的数据做出错误的判断，但模型不会对主任务产生影响。

攻击者在模型的训练过程中通过某种方式对模型植入一些后门。当后门未被激发时，被攻击的模型具有和正常模型类似的表现；而当模型中埋藏的后门被攻击者激活时，模型的输出变为攻击者预先指定的标签以达到恶意的目的。

在联邦学习场景下进行后门攻击会比较困难，一个原因就是在服务端进行聚合运算时，平均化之后会很大程度消除恶意客户端模型的影响，另一个原因是由于服务端的选择机制，因为并不能保证被攻击者挟持的客户端在每一轮都能被选取，从而降低了被后门攻击的风险。（大容量模型和 no-iid 数据分布）

# 后门攻击步骤

1. 中毒样本制作（边缘案例后门攻击、分布式后门攻击、模型依赖触发器）
2. 后门模型训练（自适应攻击、神经毒素）
3. 后门模型上传（模型替换）

# 中毒数据生成

### 标签翻转

直接修改数据集中样本的标签

### 植入触发器

原始图像 `$x$`，触发器 `$\delta$`，掩码 `$m$`，中毒图像：

`$\hat{x} = x \odot (1 - m) + \delta \odot m$`

### 语义后门

将攻击者选择的标签分配给具有某些语义特征（稀有特征）的所有样本，不需要攻击者在推理时修改模型的输入。

# 后门攻击训练策略

带有后门攻击行为的联邦学习，其客户端可以分为恶意客户端和正常客户端。不同类型的客户端，其本地训练策略各不相同。

### 正常客户端训练

正常客户端的训练过程如下，其执行过程就是常规的梯度下降过程。

正常客户端 i 第 t 轮的训练过程：

---

input:&ensp;全局模型 `$G^t$`  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;学习率: `$\eta$`;  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;本地迭代次数: `$E$`;  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;每一轮训练的样本大小: `$B$`;  
output: 返回模型更新：`$\Delta_i^t$`

利用服务端下发的全局模型参数 `$G^t$`，更新本地模型 `$L_i^t$`：`$L_i^t \leftarrow G^t$`  
for 对每一轮的迭代 i = 1, 2, 3, ..., E，执行下面的操作 do  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;将本地数据切分为大小 B 的 `$\mathcal{B}$` 份  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;for 对每一个 batch `$b \in \mathcal{B}$`  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;执行梯度下降：`$L_i^t \leftarrow L_i^t-\eta\Delta\ell(L_i^t;b)$`  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;计算更新：`$\Delta_i^t=L_i^t-G^t$`  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;end  
end  

---

### 恶意客户端训练

对于恶意客户端的本地训练，主要体现在两个方面：损失函数的设计和上传服务端的模型权重。

+  对于损失函数的设计，恶意客户端训练的目标，一方面是保证在正常数据集和被篡改毒化的数据集中都取得较好的性能；另一方面是保证本地训练的模型与全局模型之间的距离尽量小（距离越小，被服务端判断为异常模型的概率就越小）。 
+  对于上传服务端的模型权重，根据以下公式：`$L_m^{t+1} \approx \frac{n}{\eta}(X-G^t)+G^t$` 可以看出，通过增大异常客户端 `$m$` 的模型权重，使其在后面的聚合过程中，对全局模型的影响和贡献尽量持久。

恶意客户端 j 第 t 轮的训练过程:

---

input:&ensp;全局模型 `$G^t$`  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;学习率: `$\eta$`;  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;本地迭代次数: `$E$`;  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;每一轮训练的样本大小: `$B$`;  
output: 返回模型更新：`$\Delta_j^t$`

利用服务端下发的全局模型参数 `$G^t$`，更新本地模型 `$L_j^t$`：`$L_j^t \leftarrow G^t$`  
损失函数：`$\ell = \ell_{class\_loss} + \ell_{distance\_loss} / \ell_{ano\_loss}$`  
for 对每一轮的迭代 i = 1, 2, 3, ..., E，执行下面的操作 do  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;将本地数据切分为大小 B 的 `$\mathcal{B}$` 份  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;for 对每一个 batch `$b\in \mathcal{B}$`  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;数据集 `$b=\left\{ D_{cln}^m, D_{adv}^m \right\}$` 中包含正常的数据集 `$D_{cln}^m$` 和被篡改毒化的数据集 `$D_{adv}^m $`  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;执行梯度下降：`$L_j^t \leftarrow L_j^t-\eta\Delta\ell(L_j^t;b)$`  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;计算更新：`$\Delta_i^t=L_j^t-G^t$`  
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;end  
end  

---

# 分类

### 数据中毒

利用中毒数据率，在攻击影响和攻击隐蔽性之间进行平衡。

优点：简单，需要攻击者的能力少。

缺点：

### 模型中毒



# 方向

联邦学习中后门攻击的防御

# 问题

### 防御

1. 与安全聚合兼容？（[3][5]）
2. 降低主任务的性能？（[2][4][6][8][12]）
3. 被自适应攻击绕过？

### 攻击

1. 恶意客户端的影响会被良性客户端冲淡

# 影响因素

1. 攻击者数量
2. 中毒率
3. 数据分布
4. 辅助信息
5. 客户端是不是静态的（客户端的良性和恶性变化的）

# 想法

1. 深度学习在数据缺失情况下的后门防御（黑盒防御）？
2. 改进深度学习中后门防御的方法，使其适用于联邦学习中后门攻击的防御？
3. 攻击时间（早期轮次攻击和收敛轮次攻击）
4. 后门攻击更新的坐标
5. 模型容量对后门攻击的影响
6. P2P 和 纵向联邦学习中后门攻击和防御
7. 其他任务的攻击与防御（推荐系统 语音识别 自然语言处理）
8. 训练后的防御

# 防御方法

### 鲁棒聚合算法

1. Krum
2. Bulyan
3. TrimmedMean
4. Auror
5. RFA
6. FoolsGold

### 基于特征的异常检测

# 论文

### 后门攻击

1. [How to Backdoor Federated Learning](https://caiji-bai.github.io/posts/2022/05/15/How-to-Backdoor-Federated-Learning.html){:target="_blank"}
2. [Attack of the Tails Yes, You Really Can Backdoor Federated Learning](https://caiji-bai.github.io/posts/2022/05/24/Attack-of-the-Tails-Yes,-You-Really-Can-Backdoor-Federated-Learning.html){:target="_blank"}
3. [DBA: Distributed Backdoor Attacks against Federated Learning](https://caiji-bai.github.io/posts/2022/07/16/DBA-Distributed-Backdoor-Attacks-against-Federated-Learning.html){:target="_blank"}
4. [Neurotoxin_Durable Backdoors in Federated Learning](https://caiji-bai.github.io/posts/2022/07/30/Neurotoxin_Durable-Backdoors-in-Federated-Learning.html){:target="_blank"}
5. [Coordinated Backdoor Attacks against Federated Learning with Model-Dependent Triggers](https://caiji-bai.github.io/posts/2022/08/30/Coordinated-Backdoor-Attacks-against-Federated-Learning-with-Model-Dependent-Triggers.html){:target="_blank"}
6. [A highly efficient, confidential, and continuous federated learning backdoor attack strategy](https://caiji-bai.github.io/posts/2022/08/27/A-highly-efficient,-confidential,-and-continuous-federated-learning-backdoor-attack-strategy.html){:target="_blank"}

[1] 利用客户端可以直接影响全局模型权重的特点，在模型收敛时发起模型替换攻击。

[2] 攻击不具代表性的样本进行后门攻击（标签翻转）

[3] 利用联邦学习分布式的特点，将触发器分给不同的客户端

[4] 攻击模型中不具代表性的坐标进行后门攻击

[5] 将模型依赖触发器与 DBA 结合

### 防御

1. [Can You Really Backdoor Federated Learning?](https://caiji-bai.github.io/posts/2022/05/21/Can-You-Really-Backdoor-Federated-Learning.html){:target="_blank"}
2. [Defending against Backdoors in Federated Learning with Robust Learning Rate](https://caiji-bai.github.io/posts/2022/05/31/Defending-against-Backdoors-in-Federated-Learning-with-Robust-Learning-Rate.html){:target="_blank"}
3. [BaFFLe: Backdoor Detection via Feedback-based Federated Learning](https://caiji-bai.github.io/posts/2022/07/19/BaFFLe-Backdoor-Detection-via-Feedback-based-Federated-Learning.html){:target="_blank"}
4. [FLAME: Taming Backdoors in Federated Learning](https://caiji-bai.github.io/posts/2022/07/26/FLAME_Taming-Backdoors-in-Federated-Learning.html){:target="_blank"}
5. [Meta Federated Learning](https://caiji-bai.github.io/posts/2022/07/12/Meta-Federated-Learning.html)
6. [Against Backdoor Attacks In Federated Learning With Differential Privacy](https://caiji-bai.github.io/posts/2022/08/02/Against-Backdoor-Attacks-In-Federated-Learning-With-Differential-Privacy.html){:target="_blank"}
7. [Resisting Distributed Backdoor Attacks in Federated Learning_A Dynamic Norm Clipping Approach](https://caiji-bai.github.io/posts/2022/09/03/Resisting-Distributed-Backdoor-Attacks-in-Federated-Learning_A-Dynamic-Norm-Clipping-Approach.html){:target="_blank"}
8. [FederatedReverse: A Detection and Defense Method Against Backdoor Attacks in Federated Learning](https://caiji-bai.github.io/posts/2022/09/06/FederatedReverse_A-Detection-and-Defense-Method-Against-Backdoor-Attacks-in-Federated-Learning.html){:target="_blank"}
9. [BatFL_Backdoor Detection on Federated Learning in e-Health](https://caiji-bai.github.io/posts/2022/09/10/BatFL_Backdoor-Detection-on-Federated-Learning-in-e-Health.html){:target="_blank"}
10. [Backdoor attacks-resilient aggregation based on Robust Filtering of Outliers in federated learning for image classification](https://caiji-bai.github.io/posts/2022/09/13/Backdoor-attacks-resilient-aggregation-based-on-Robust-Filtering-of-Outliers-in-federated-learning-for-image-classification.html){:target="_blank"}
11. [Defense against backdoor attack in federated learning](https://caiji-bai.github.io/posts/2022/09/20/Defense-against-backdoor-attack-in-fe-derate-d-learning.html){:target="_blank"}
12. [Mitigating the Backdoor Attack by Federated Filters for Industrial IoT Applications](https://caiji-bai.github.io/posts/2022/10/01/Mitigating-the-Backdoor-Attack-by-Federated-Filters-for-Industrial-IoT-Applications.html){:target="_blank"}

### 深度学习 -> 联邦学习

1. [Trojaning Attack on Neural Networks] -> [Coordinated Backdoor Attacks against Federated Learning with Model-Dependent Triggers]
2. []

# 问题

1. 为什么联邦学习的任务都集中在图像分类和 NLP 领域？

目前联邦学习的应用领域：文本预测、语音识别、人脸识别、医疗

后门攻击的领域：文本预测、图像识别、贷款状态预测

2. 分布式机器学习安全问题？



# 实验部分

### 划分 no-iid 的方法

1. 用具有不同超参数 `$\alpha$` 的狄利克雷分布来生成不同的数据分布。


### 比较方法

1. 鲁棒联邦学习聚合算法：RFA、FoolsGold


### 评估指标

攻击成功率：`$ASR_M = \frac{n_T}{n}\times 100$`

### 工具箱

[backdooor101](https://github.com/ebagdasa/backdoors101)

# 后门攻击原理

后门攻击利用的是机器学习算法中的一大关键特征，即模型会无意识在训练数据中搜索强相关性，而无需明确其背后的因果关系。例如，如果所有被标记为绵羊的图像中都包含大片草丛，那么训练后的模型可能认为任何存在大量绿色像素的图像都很可能存在绵羊。同样的，如果某个类别下的所有图像都包含相同的对抗触发器，则模型很可能会把是否存在触发器视为当前标签的强相关因素。




# 基础知识

### 什么是自适应攻击？
