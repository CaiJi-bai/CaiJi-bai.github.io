---
layout: mypost
title: Coordinated Backdoor Attacks against Federated Learning with Model-Dependent Triggers
categories: [论文阅读]
---

<table border="1">
    <tr>
        <th>论文英文名字</th>
        <th>Coordinated Backdoor Attacks against Federated Learning with Model-Dependent Triggers</th>
    </tr>
    <tr>
        <th>论文中文名字</th>
        <th>联邦学习</th>
    </tr>
    <tr>
        <td>作者</td>
        <td>Xueluan Gong, Yanjiao Chen, Huayang Huang, Yuqing Liao, Shuai Wang, Qian Wang</td>
    </tr>
    <tr>
        <td>来源</td>
        <td>IEEE Network, Volume 36 [会议等级]</td>
    </tr>
    <tr>
        <td>年份</td>
        <td>2020 年 10 月</td>
    </tr>
    <tr>
        <td>作者动机</td>
        <td>解决问题</td>
    </tr>
    <tr>
        <td>阅读动机</td>
        <td>经典阅读（可以详细谈一下）</td>
    </tr>
    <tr>
        <td>创新点</td>
        <td>双掩码</td>
    </tr>
</table>

# 内容总结

# 主要贡献

1. 使用多个本地后门触发器设计了针对联邦学习的有效后门攻击，实现了高攻击成功率和主要任务准确性。 
2. 将提出的攻击与三个基线进行比较，包括单触发攻击（例如，使用随机或模型相关的触发器）和协调触发攻击（例如，使用随机触发器）。

# 预备知识

### 随机触发器

随机触发器是独立于学习模型生成的。例如，徽标，贴纸或像素扰动。由于随机触发器与模型训练过程不相关，因此我们发现其在触发后门模型的后门方面的能力较弱，尤其是在聚合过程中局部模型被稀释的联邦学习中。

### 模型依赖触发器

模型依赖触发器被设计成最大限度地激活学习模型内的某些神经元。我们发现全局模型更有可能记住聚合后的触发器。

# 威胁模型



# 攻击方法

### 概述

本文提出了一种使用模型依赖触发器针对联邦学习的优化的协调后门攻击。它包括两个主要步骤：模型依赖局部触发器的生成和协调后门注入。

![联邦学习中协同后门攻击的概述](联邦学习中协同后门攻击的概述.png)

### 模型依赖局部触发器的生成

1. 确定触发器的结构，包括其形状，大小和位置。
2. 当模型接近收敛时，每个恶意用户端构建触发器掩码 `$M$`。`$M$` 是一个布尔值矩阵，与模型的输入具有相同的维数。对于输入数据，掩模 `$M$` 将触发器区域初始化为随机值，其他部分初始化为 0。触发器生成过程相当于寻找掩码的最优值赋值的过程。
3. 将标签是目标标签的大量数据样本输入到模型中，在神经网络中选择一层，并记录所选层中每个神经元的激活数量，最终选择激活次数和到下一层的权重值总和最大的神经元。
4. 对于选定的神经元，定义一个损失函数，为当前神经元的值与其目标值之间的均方误差，并使用梯度下降法使得损失函数最小化，其中目标值为所选层中神经元的最大值。
5. 在最后一轮迭代之后，从掩码中提取触发器。
6. 生成局部触发器后，恶意客户端对局部训练数据进行投毒。
7. 恶意客户端使用有毒的数据集将接收到的全局模型从选定层重新训练到输出层，获得后门局部模型。
8. 为了增强攻击能力，恶意客户端需要在向服务器提交本轮更新之前，放大恶意更新参数。其中放大因子的选择需要在主任务准确率与攻击成功率之间进行权衡。
9. 上述过程投毒过程将重复多次，直到全局模型收敛。
10. 最后，恶意客户端可以使用他们的本地触发器或由本地触发器组成的全局触发器来触发嵌入在全局模型中的后门。

### 协调后门注入

# 实验

+ 数据集：MNIST 和 CIFAR-10


### 实验结果


# 结论



# 参考

1. [百度](https://www.baidu.com){:target="_blank"}