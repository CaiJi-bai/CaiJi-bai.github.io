---
layout: mypost
title: Hidden Trigger Backdoor Attacks
categories: [论文阅读]
---

<table border="1">
    <tr>
        <th>论文英文名字</th>
        <th>Hidden Trigger Backdoor Attacks</th>
    </tr>
    <tr>
        <th>论文中文名字</th>
        <th>隐藏的触发器后门攻击</th>
    </tr>
    <tr>
        <td>作者</td>
        <td>Aniruddha Saha, Akshayvarun Subramanya, Hamed Pirsiavash</td>
    </tr>
    <tr>
        <td>来源</td>
        <td>34th AAAI 2020: New York, NY, USA [CCF 人工智能 A 类会议]</td>
    </tr>
    <tr>
        <td>年份</td>
        <td>2020 年 4 月</td>
    </tr>
    <tr>
        <td>作者动机</td>
        <td>大多数最先进的后门攻击要么提供可以通过目视检查识别的错误标记的中毒数据，要么显示中毒数据中的触发器，要么使用噪声隐藏触发器。</td>
    </tr>
    <tr>
        <td>阅读动机</td>
        <td></td>
    </tr>
    <tr>
        <td>创新点</td>
        <td>使中毒的图像和目标类的图像相似，将触发器保密到测试时间。</td>
    </tr>
</table>

# 内容总结

# 主要贡献

1. 提出了隐藏的触发器后门攻击，即中毒数据被正确标记，也不包含任何可见的触发器，因此，受害者不容易通过目视检查来识别中毒数据。
2. 进行了各种实验和消融研究。

# 方法

# 原来的中毒数据生成方法

将触发器添加到来自源类的一组图像并将其标签更改为目标类来生成一组中毒训练数据 (图像和标签对)。

给定来自源类别的源图像 `$s_i$`、触发器补丁 `$P$` 和在补丁的位置为 1 而在其他任何地方为 0 的二进制掩码 `$M$`，攻击者将触发器粘贴在源图像上以获得修补的源图像 `$\tilde{s}$`：

`$\tilde{s}_i=s_i\odot(1-m)+p\odot m$`

# 新的中毒数据生成方法

给定一个目标图像 `$t$`、一个源图像 `$s$` 和一个触发补丁 `$p$`，我们将触发粘贴到 `$s$` 上，以使用等式 1 获得补丁后的源图像 `$\tilde{s}$`。然后我们通过解决以下优化来优化中毒图像 `$z$`：

![公式2](公式2.png)

给定源和目标类的一对图像以及触发器的一个固定位置，以上优化将产生一个单独的数据点。

跨源图像和触发位置的泛化：使后门模型其适用于新的源图像（中毒时未见的图像）和任何随机位置的触发器。

![算法1](算法1.png)

# 实验

+ 数据集：ImageNet
+ 训练模型：AlexNet

**ImageNet随机对**

**ImageNet手工挑选对**

**ImageNet“狗”对**

![随机配对、手工挑选配对以及只有狗配对的结果](随机配对、手工挑选配对以及只有狗配对的结果.png)

**ImageNet随机对的消融研究**

![消融研究的结果](消融研究的结果.png)

# 总结

我们**提出了一种新颖的后门攻击，它是通过在测试时在看不见的图像上的随机位置添加一个小补丁来触发的。带有干净标签的中毒数据看起来很自然，并且不会显示触发因素。因此，攻击者可以将触发器保密，直到实际攻击时间。**我们表明我们的攻击适用于两个不同的数据集和各种设置。我们还表明，最先进的后门检测方法不能有效地防御我们的攻击。我们认为，此类实际攻击揭示了深度学习算法的一个重要漏洞，在对手存在的情况下，在关键的现实世界应用程序中部署深度学习算法之前需要解决该漏洞。我们希望本文有助于进一步研究开发更好的防御模型。