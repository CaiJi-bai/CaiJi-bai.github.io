---
layout: mypost
title: Attack of the Tails Yes, You Really Can Backdoor Federated Learning
categories: [论文阅读]
---

<table border="1">
    <tr>
        <th>论文英文名字</th>
        <th>Attack of the Tails: Yes, You Really Can Backdoor Federated Learning</th>
    </tr>
    <tr>
        <th>论文中文名字</th>
        <th>尾部的攻击：是的，你真的可以后门联邦学习</th>
    </tr>
    <tr>
        <td>作者</td>
        <td>Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jy-yong Sohn, Kangwook Lee, Dimitris S. Papailiopoulos</td>
    </tr>
    <tr>
        <td>来源</td>
        <td>34th NeurIPS 2020</td>
    </tr>
    <tr>
        <td>年份</td>
        <td>2020 年 12 月</td>
    </tr>
    <tr>
        <td>作者动机</td>
        <td>目前联邦学习系统是否对后门具有鲁棒性仍然是一个悬而未决的问题</td>
    </tr>
    <tr>
        <td>阅读动机</td>
        <td></td>
    </tr>
    <tr>
        <td>创新点</td>
        <td>提出边缘案例后门攻击方案</td>
    </tr>
</table>

# 内容总结  

# 主要贡献

1. 如果一个模型容易受到对抗性示例 [28-32] 形式的推理时攻击，那么在温和的条件下，相同的模型将容易受到后门训练时攻击。
2. 提出边缘案例后门（edge-case backdoor）
3. 实验证明后门攻击的有效性

# 边缘案例后门

首先正式定义一个 p-edge-case 示例集如下:

![定义1](定义1.png)

p 值较小的 p-edge-case 可以看作是一组标记示例，其中输入特征是从特征分布的重尾中选择的。 请注意，我们对标签没有任何条件，即可以考虑任意标签。

如何构造一个 p-edge-case 示例集？

假设对手有一组候选的边缘案例样本和一些良性样本。用良性样本提供一个预训练的预测模型，并收集最后一层的输出向量。通过拟合一个聚类数等于类数的高斯混合模型，我们可以获得一个生成模型，攻击者可以使用该模型测量给定样本的概率密度，并在需要时过滤掉。

![可视化对数概率密度](可视化对数概率密度.png)

![边缘案例示例](边缘案例示例.png)

图 1：我们的后门的任务说明和边缘案例示例。 请注意，在相应数据集的训练/测试中找不到这些示例。  (a) 标有“卡车”的西南飞机为 CIFAR-10 分类器安装后门。  (b) 来自 ARDIS 数据集的“7”图像标记为“1”，以作为 MNIST 分类器的后门。  (c) 穿着传统克里特服饰的人被错误地标记为后门 ImageNet 分类器（故意模糊）。  (d) 导演 Yorgos Lanthimos (YL) 上的正面推文被标记为“负面”，以对情绪分类器进行后门。  （e）关于雅典的句子以带有负面含义的词完成，以便为下一个词预测器提供后门。

根据访问模型提出三种不同的攻击策略：
1. 黑盒攻击：攻击者在本地制作的数据集 `$D' = D \cup D_{edge}$` 上执行标准的本地训练，无需修改，旨在最大化 D' 上的全局模型的准确性。
2. PGD 攻击：攻击者对 `$D' = D \cup D_{edge}$` 的损失应用投影梯度下降，约束条件是局部模型不会偏离全局模型太多。
3. 模型替换的 PGD 攻击：该策略结合了 (2) 中的过程和 [13] 中的模型替换攻击，其中模型参数在发送到服务器之前被缩放，以抵消其他诚实的贡献节点。

# 实验

+ 数据集：CIFAR 10/ImageNet/EMNIST/Reddit/Sentiment 140
+ 模型：VGG-9/VGG-11/LeNet/LSTMs
+ 评估指标：测试准确率
+ 攻击者参与模式：固定频率和固定池
+ 防御手段：范数差异裁剪、KRUM、MULTI-KRUM、RFA 和弱差分隐私

![图4](图4.png)

+ 攻击的有效性随着我们允许诚实的客户使用更多的 `$D_{edge}$` 而下降，证明了纯边缘情况攻击是最强的。

![图5](图5.png)

图 5：黑盒、带模型替换的 PGD、不带模型替换的 PGD 攻击在任务 1（顶部）和任务 4（底部）的各种防御下的有效性，每 10 轮有一个对手。 误差线代表 3 个独立实验试验的一个标准偏差。

+ 经过精心调整的范数约束的白盒攻击 (有/没有替换) 可以绕过所有考虑的防御
+ KRUM 甚至加强了攻击，因为它可能会忽略诚实更新但接受后门更新。

![图6](图6.png)

图 6：针对边缘案例攻击的防御方法的潜在公平性问题：（a）KRUM 选择的客户端频率和（b）MULTI-KRUM；  (c) 测试主要任务、目标任务、带有干净标签的边缘案例示例（例如西南示例的“飞机”）和原始 CIFAR-10 飞机类任务的准确性。

+ KRUM 从不使用具有西南航空示例的客户端的模型更新
+ 尽管 MULTI-Krum 从西南航空公司示例的客户处选择模型更新，但这是由于其拒绝每一 FL 轮次的模型更新很少，即当多个西南航空示例的诚实客户出现在同一 FL 轮次时，MULTI-Krum 不会拒绝其所有模型更新。与其他客户相比，选择西南地区客户的频率要少得多。
+ 在聚合模型上添加噪声可以防御后门攻击
+ 总体测试集图像和原始 CIFAR-10 飞机的精度都下降了，西南飞机的准确性下降幅度超过了原始任务



# 总结

在本文中，我们提出了支持存在难以检测和防御的后门 FL 攻击的理论和实验证据。我们引入了边缘案例后门攻击，目标是预测子任务，这些子任务不太可能在训练或测试数据集中找到，但很自然。 这些边缘案例后门的有效性和持久性表明，在当前形式下，联邦学习系统容易受到对抗性代理的影响，突出了当前鲁棒性保证的不足。


